{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb025a2-375b-4815-8278-df4f2d22b831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c6cc48e-5cec-4d23-a41f-7226746a4b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ea321b-1110-4404-bcc8-72e9c6e69347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "32 fits failed out of a total of 64.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.485398   0.48237957 0.485398   0.48237957\n",
      " 0.485398   0.48237957 0.485398   0.48237957        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56427096 0.56493778 0.56468321 0.5648552  0.56471625 0.56477776\n",
      " 0.56471625 0.56475822]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     37\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test_preprocessed)\n\u001b[0;32m---> 39\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(\u001b[43my_test\u001b[49m, y_pred))\n\u001b[1;32m     40\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(y_test, y_pred)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "y_train = train_data['Rings']\n",
    "X_train = train_data.drop(['id', 'Rings'], axis=1)\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ordinal_encoding', OrdinalEncoder(), make_column_selector(dtype_include='object')),\n",
    "    ('scale', StandardScaler(), make_column_selector(dtype_include='float64'))\n",
    "])\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define the reduced hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create the Random Forest Regressor model\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(rf_regressor, param_grid=param_grid, cv=2, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Get the best model and evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_preprocessed)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286f491f-422b-462c-8fd0-8d88c41a3a04",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Rings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Rings'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Define y_test\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m     24\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordinal_encoding\u001b[39m\u001b[38;5;124m'\u001b[39m, OrdinalEncoder(), make_column_selector(dtype_include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m     25\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(), make_column_selector(dtype_include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     26\u001b[0m ])\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Rings'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "y_train = train_data['Rings']\n",
    "X_train = train_data.drop(['id', 'Rings'], axis=1)\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Define y_test\n",
    "y_test = test_data['Rings']\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ordinal_encoding', OrdinalEncoder(), make_column_selector(dtype_include='object')),\n",
    "    ('scale', StandardScaler(), make_column_selector(dtype_include='float64'))\n",
    "])\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define the reduced hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create the Random Forest Regressor model\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(rf_regressor, param_grid=param_grid, cv=2, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Get the best model and evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_preprocessed)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d55076-0804-46d7-9a7b-aed340bc828b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "32 fits failed out of a total of 64.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.48879181 0.48538506 0.48879181 0.48538506\n",
      " 0.48879181 0.48538506 0.48879181 0.48538506        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56646029 0.56676618 0.56647481 0.56676294 0.56701419 0.56664674\n",
      " 0.56735399 0.56699791]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.13\n",
      "R-squared: 0.56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = train_data.drop(['id', 'Rings'], axis=1)\n",
    "y = train_data['Rings']\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ordinal_encoding', OrdinalEncoder(), make_column_selector(dtype_include='object')),\n",
    "    ('scale', StandardScaler(), make_column_selector(dtype_include='float64'))\n",
    "])\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train1)\n",
    "X_test_preprocessed = preprocessor.transform(X_test1)\n",
    "\n",
    "# Define the reduced hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create the Random Forest Regressor model\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(rf_regressor, param_grid=param_grid, cv=2, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_preprocessed, y_train1)\n",
    "\n",
    "# Get the best model and evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_preprocessed)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5dde4-4a6e-4193-9856-7c56d1adc9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "93 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "93 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [        nan -4.10635271 -4.34919257 -4.04621996         nan         nan\n",
      " -3.60173168         nan         nan         nan -3.84809452         nan\n",
      "         nan -4.05698013         nan -4.72344649 -3.59780715 -3.84803224\n",
      " -3.68742803 -4.98353513 -4.11168052         nan -4.04900537         nan\n",
      " -3.57487285 -3.68983322 -4.48658369 -3.59989333         nan         nan\n",
      " -3.51023158 -3.84008796 -4.04352351         nan -4.10504778 -5.16621402\n",
      " -4.4823525  -5.16453778 -4.47687318 -5.01835964 -4.05176065         nan\n",
      " -4.0545584  -4.11928063 -3.84078546         nan -4.09984827 -3.84417868\n",
      " -3.59724429         nan -3.60033003         nan -5.17167645 -4.10026683\n",
      " -4.10566858 -3.84598063 -3.70456668         nan -4.72012551         nan\n",
      " -3.68309563 -5.0046024  -3.69882606 -4.34783553         nan -3.84367308\n",
      " -4.04616308         nan -3.6794682          nan -4.35061357 -3.69789808\n",
      " -4.48789071 -4.34550486         nan -3.84144766         nan         nan\n",
      " -3.5786851  -4.75312667 -4.72074439         nan -3.67724651 -4.0540764\n",
      " -3.84299865 -3.5083364  -3.57612035 -4.73211756 -3.69993144         nan\n",
      " -3.67505901         nan -3.68120635         nan -5.16903056 -3.84292725\n",
      " -4.33633192 -3.60409508         nan -3.84389143]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.91\n",
      "R-squared: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the Random Forest Regressor model\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV with 3-fold cross-validation\n",
    "random_search = RandomizedSearchCV(rf_regressor, param_distributions=param_dist, n_iter=100, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1, verbose=1)\n",
    "random_search.fit(X_train_preprocessed, y_train1)\n",
    "\n",
    "# Get the best model and evaluate on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_preprocessed)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19db520b-1af6-487a-aed5-d5df9ec68eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.88\n",
      "R-squared: 0.65\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = train_data.drop(['id', 'Rings'], axis=1)\n",
    "y = train_data['Rings']\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ordinal_encoding', OrdinalEncoder(), make_column_selector(dtype_include='object')),\n",
    "    ('scale', StandardScaler(), make_column_selector(dtype_include='float64'))\n",
    "])\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Create the XGBoost Regressor model\n",
    "xgb_regressor = XGBRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_regressor.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b00fc4-a9f5-40b7-9a7a-203dfbbcf180",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1331\n",
      "[LightGBM] [Info] Number of data points in the train set: 72492, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 9.697111\n",
      "RMSE: 1.88\n",
      "R-squared: 0.66\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = train_data.drop(['id', 'Rings'], axis=1)\n",
    "y = train_data['Rings']\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ordinal_encoding', OrdinalEncoder(), make_column_selector(dtype_include='object')),\n",
    "    ('scale', StandardScaler(), make_column_selector(dtype_include='float64'))\n",
    "])\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Create the LGBM Regressor model\n",
    "lgbm_regressor = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lgbm_regressor.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lgbm_regressor.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68e2fe6a-ff56-4e63-8a56-2e9140320b73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1331\n",
      "[LightGBM] [Info] Number of data points in the train set: 72492, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 9.697111\n",
      "RMSLE Score (LGBM): 0.15276044735952493\n",
      "RMSLE Score (XGBoost): 0.15323146963732945\n",
      "60411\n",
      "18123\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = train_data.drop(['id', 'Rings'], axis=1)\n",
    "y = train_data['Rings']\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ordinal_encoding', OrdinalEncoder(), make_column_selector(dtype_include='object')),\n",
    "    ('scale', StandardScaler(), make_column_selector(dtype_include='float64'))\n",
    "])\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define the LGBM Regressor model\n",
    "lgbm_regressor = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the LGBM model\n",
    "lgbm_regressor.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict on the test set using LGBM model\n",
    "y_pred_lgbm = lgbm_regressor.predict(X_test_preprocessed)\n",
    "\n",
    "# Define the XGBoost Regressor model\n",
    "xgb_regressor = XGBRegressor(random_state=42)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_regressor.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict on the test set using XGBoost model\n",
    "y_pred_xgb = xgb_regressor.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluate the models using RMSLE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmsle_lgbm = np.sqrt(mean_squared_error(np.log1p(y_pred_lgbm), np.log1p(y_test)))\n",
    "rmsle_xgb = np.sqrt(mean_squared_error(np.log1p(y_pred_xgb), np.log1p(y_test)))\n",
    "\n",
    "print(f'RMSLE Score (LGBM): {rmsle_lgbm}')\n",
    "print(f'RMSLE Score (XGBoost): {rmsle_xgb}')\n",
    "\n",
    "print(len(test_data['id']))  # Check the length of the 'id' column\n",
    "print(len(y_pred_lgbm))  # Check the length of the predictions array\n",
    "\n",
    "#submission = pd.DataFrame({'id': test_data['id'], 'Rings': y_pred_lgbm})\n",
    "\n",
    "# Save predictions to a CSV file following Kaggle submission format\n",
    "#submission.to_csv('submission1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdb76862-d31f-4d96-8ca7-429861ee5d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions using LightGBM model\n",
    "test_data['Predictions_LGBM'] = lgbm_regressor.predict(preprocessor.transform(test_data.drop(['id'], axis=1)))\n",
    "\n",
    "# Predictions using XGBoost model\n",
    "test_data['Predictions_XGB'] = xgb_regressor.predict(preprocessor.transform(test_data.drop(['id'], axis=1)))\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "test_data[['id', 'Predictions_LGBM', 'Predictions_XGB']].to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6811becb-5533-4a4c-b964-d0c4b8be7796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1335\n",
      "[LightGBM] [Info] Number of data points in the train set: 90615, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 9.696794\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [60411, 18123]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Predict on the test set using LGBM model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y_pred_lgbm \u001b[38;5;241m=\u001b[39m lgbm_regressor\u001b[38;5;241m.\u001b[39mpredict(X_test_preprocessed)\n\u001b[0;32m---> 26\u001b[0m rmsle_lgbm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog1p\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_lgbm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog1p\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSLE Score (LGBM): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmsle_lgbm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for submission\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    495\u001b[0m         )\n\u001b[0;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    104\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [60411, 18123]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X_train = train_data.drop(['id', 'Rings'], axis=1)\n",
    "y_train = train_data['Rings']\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Preprocess the data (assuming you've already fitted the preprocessor)\n",
    "X_train_preprocessed = preprocessor.transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define the LGBM Regressor model\n",
    "lgbm_regressor = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the LGBM model\n",
    "lgbm_regressor.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict on the test set using LGBM model\n",
    "y_pred_lgbm = lgbm_regressor.predict(X_test_preprocessed)\n",
    "\n",
    "rmsle_lgbm = np.sqrt(mean_squared_error(np.log1p(y_pred_lgbm), np.log1p(y_test)))\n",
    "\n",
    "\n",
    "print(f'RMSLE Score (LGBM): {rmsle_lgbm}')\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'Rings': y_pred_lgbm})\n",
    "\n",
    "# Save predictions to a CSV file following Kaggle submission format\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52256c-b6b0-4645-9878-9bebc5f94919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
